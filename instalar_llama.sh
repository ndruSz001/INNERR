#!/bin/bash
# Script de instalaciÃ³n y configuraciÃ³n de Llama.cpp para TARS
# Optimizado para RTX 3050/3060

set -e

echo "ðŸš€ INSTALACIÃ“N DE LLAMA.CPP PARA TARS"
echo "====================================="
echo "Este script configurarÃ¡ Llama.cpp como backend optimizado"
echo "para tu RTX 3050/3060 con cuantizaciÃ³n Q4_K_M"
echo ""

# Verificar sistema
echo "ðŸ” Verificando sistema..."
if [[ "$OSTYPE" == "linux-gnu"* ]]; then
    echo "âœ… Linux detectado"
elif [[ "$OSTYPE" == "darwin"* ]]; then
    echo "âœ… macOS detectado"
else
    echo "âŒ Sistema operativo no soportado"
    exit 1
fi

# Verificar GPU
echo "ðŸ” Verificando GPU..."
if command -v nvidia-smi &> /dev/null; then
    GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits)
    echo "âœ… NVIDIA GPU detectada: $GPU_INFO"
else
    echo "âš ï¸ No se detectÃ³ GPU NVIDIA. El rendimiento serÃ¡ limitado."
fi

# Instalar dependencias
echo "ðŸ“¦ Instalando dependencias..."
if [[ "$OSTYPE" == "linux-gnu"* ]]; then
    sudo apt update
    sudo apt install -y build-essential cmake git
elif [[ "$OSTYPE" == "darwin"* ]]; then
    if ! command -v brew &> /dev/null; then
        echo "âŒ Homebrew no encontrado. InstÃ¡lalo desde https://brew.sh/"
        exit 1
    fi
    brew install cmake git
fi

# Clonar y compilar llama.cpp
echo "ðŸ”§ Clonando y compilando Llama.cpp..."
if [ ! -d "llama.cpp" ]; then
    git clone https://github.com/ggerganov/llama.cpp
fi

cd llama.cpp

# Compilar con optimizaciones para RTX 30xx
echo "âš¡ Compilando con optimizaciones CUDA..."
if command -v nvcc &> /dev/null; then
    # Con CUDA
    make LLAMA_CUDA=1 LLAMA_CUDA_FORCE_MMQ=1 LLAMA_CUDA_FORCE_CUBLAS=1
else
    # Sin CUDA (CPU only)
    make
fi

echo "âœ… Llama.cpp compilado exitosamente"

# Crear directorio para modelos
cd ..
mkdir -p modelos

echo ""
echo "ðŸ“‹ PRÃ“XIMOS PASOS:"
echo "=================="
echo ""
echo "1. Descarga el modelo Phi-2:"
echo "   huggingface-cli download microsoft/phi-2 --local-dir modelos/phi-2"
echo ""
echo "2. Convierte el modelo a GGUF:"
echo "   cd llama.cpp"
echo "   python convert-hf-to-gguf.py ../modelos/phi-2/"
echo ""
echo "3. Cuantiza a Q4_K_M (Ã³ptimo para RTX 30xx):"
echo "   ./quantize ../modelos/phi-2.gguf ../modelos/phi-2-q4_k_m.gguf Q4_K_M"
echo ""
echo "4. Prueba el rendimiento:"
echo "   ./main -m ../modelos/phi-2-q4_k_m.gguf --prompt \"Hola TARS\" -n 50 --gpu-layers 35"
echo ""
echo "5. Integra con TARS:"
echo "   python optimizacion_llama.py test"
echo ""

echo "ðŸŽ¯ CONFIGURACIÃ“N RECOMENDADA PARA RTX 3050/3060:"
echo "- gpu-layers: 35 (aprox. 4GB VRAM)"
echo "- threads: 8-12 (nÃºcleos de CPU)"
echo "- ctx-size: 2048 (contexto de conversaciÃ³n)"
echo "- temp: 0.7-0.8 (creatividad balanceada)"
echo ""

echo "ðŸ“Š RENDIMIENTO ESPERADO:"
echo "- Primera respuesta: ~2-3 segundos"
echo "- Respuestas siguientes: ~0.5-1 segundo"
echo "- Uso de CPU: 40-50%"
echo "- Temperatura CPU: 60-65Â°C"
echo ""

echo "âœ… InstalaciÃ³n completada. Â¡TARS ahora volarÃ¡! ðŸš€"